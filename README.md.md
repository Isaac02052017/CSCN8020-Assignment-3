# ğŸ•¹ï¸ Deep Q-Network (DQN) for Atari Pong  
### CSCN 8020 â€” Reinforcement Learning Programming â€¢ Assignment 3

This repository contains the implementation, experiments, and analysis for a Deep Q-Network (DQN) agent trained on the **PongDeterministic-v4** Atari environment. The project evaluates the effect of changing **mini-batch size** and **target network update rate**, and reports all required metrics according to the assignment specification.

---

## ğŸ“ Repository Structure

```
ğŸ“¦ DQN-Pong-Assignment3
â”‚
â”œâ”€â”€ assignment3.ipynb
â”œâ”€â”€ assignment3_utils.py
â”‚
â”œâ”€â”€ Results/
â”‚   â”œâ”€â”€ baseline_b8_t10.csv
â”‚   â”œâ”€â”€ baseline_b8_t10_score.png
â”‚   â”œâ”€â”€ baseline_b8_t10_avg5.png
â”‚   â”œâ”€â”€ batch16_t10.csv
â”‚   â”œâ”€â”€ batch8_t3.csv
â”‚   â””â”€â”€ summary.csv
â”‚
â”œâ”€â”€ New/
â”‚   â”œâ”€â”€ mini_batch_score_comparison.png
â”‚   â”œâ”€â”€ mini_batch_avg5_comparison.png
â”‚   â”œâ”€â”€ target_update_score_comparison.png
â”‚   â””â”€â”€ target_update_avg5_comparison.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  Project Overview

The goal of this assignment is to:

- Implement a **Deep Q-Network (DQN)** for Pong  
- Modify the CNN input to use **4 stacked frames (84Ã—80)**  
- Evaluate:
  - Score per episode  
  - Average reward of last 5 episodes  
- Change two parameters independently:
  - Mini-batch size â†’ *8 vs 16*  
  - Target network update rate â†’ *every 3 vs every 10 episodes*  
- Report results and select the **best configuration**

---

## ğŸ› ï¸ Installation & Dependencies

### Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
```

### Install dependencies
```bash
pip install gymnasium[atari,accept-rom-license]
pip install autorom[accept-rom-license]
pip install torch torchvision numpy matplotlib pandas
```

---

## ğŸ® Environment & Preprocessing

### Environment
- `PongDeterministic-v4`
- 6 discrete actions
- Deterministic frame skipping

### Preprocessing (assignment3_utils.py)
- Crop unimportant regions  
- Resize to **84Ã—80**  
- Grayscale conversion  
- Normalize to [-1, +1]  
- Stack **4 frames** â†’ (4, 84, 80)

---

## ğŸ§© Neural Network Architecture

| Layer | Parameters | Output |
|-------|------------|--------|
| Input | 4 Ã— 84 Ã— 80 | (4, 84, 80) |
| Conv1 | 32 filters, 8Ã—8, stride=4 | (32, 20, 20) |
| Conv2 | 64 filters, 4Ã—4, stride=2 | (64, 9, 9) |
| Conv3 | 64 filters, 3Ã—3, stride=1 | (64, 7, 7) |
| Flatten | â€” | 3136 |
| FC1 | Dense 512 | 512 |
| FC2 | Dense 6 | Q-values |

Training:
- Loss: Smooth L1  
- Optimizer: Adam (1e-4)  
- Discount: Î³ = 0.95  
- Replay buffer: 100k  

---

## ğŸš€ Running the Notebook

```bash
jupyter notebook assignment3.ipynb
```

---

## ğŸ“Š Metrics Collected

- **Score per Episode**  
- **Average Reward (Last 5 Episodes)**  

Saved under **Results/**.

---

## ğŸ“ˆ Experiment Results

### Baseline (batch=8, target=10)
- `baseline_b8_t10_score.png`
- `baseline_b8_t10_avg5.png`

### Mini-batch comparison (8 vs 16)
- `mini_batch_score_comparison.png`
- `mini_batch_avg5_comparison.png`

### Target-update comparison (3 vs 10)
- `target_update_score_comparison.png`
- `target_update_avg5_comparison.png`

---

## ğŸ† Best Hyperparameter Combination

**Batch Size = 8**  
**Target Network Update Rate = 10 episodes**

Provides the best balance of:
- Fast early learning  
- Stable gradients  
- Smooth performance curves  

---

## ğŸ“œ Conclusion

This project demonstrates the successful implementation of DQN with stacked visual input for Pong. Experimental results show that smaller mini-batches and less frequent target updates deliver more stable and efficient learning within a short training budget.
